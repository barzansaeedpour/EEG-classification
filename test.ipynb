{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Barzan\\AppData\\Local\\Temp\\ipykernel_8656\\2410072469.py:5: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas\n"
     ]
    }
   ],
   "source": [
    "import mne\n",
    "import os\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "import pandas\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28\n"
     ]
    }
   ],
   "source": [
    "all_file_path = glob('dataset/*.edf')\n",
    "print(len(all_file_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n",
      "14\n"
     ]
    }
   ],
   "source": [
    "healthy_file_path = [i for i in all_file_path if 'h' in i.split('\\\\')[1]]\n",
    "patient_file_path = [i for i in all_file_path if 's' in i.split('\\\\')[1]]\n",
    "print(len(healthy_file_path))\n",
    "print(len(patient_file_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(file_path):\n",
    "    data = mne.io.read_raw_edf(file_path, preload = True)\n",
    "    data.set_eeg_reference()\n",
    "    data.filter(l_freq = 0.5, h_freq = 45)\n",
    "    epochs = mne.make_fixed_length_epochs(data, duration = 5, overlap = 1)\n",
    "    array = epochs.get_data()\n",
    "    return array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting EDF parameters from f:\\workspace\\EEG-classification\\dataset\\h01.edf...\n",
      "EDF file detected\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n",
      "Reading 0 ... 231249  =      0.000 ...   924.996 secs...\n",
      "EEG channel type selected for re-referencing\n",
      "Applying average reference.\n",
      "Applying a custom ('EEG',) reference.\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 0.5 - 45 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 0.50\n",
      "- Lower transition bandwidth: 0.50 Hz (-6 dB cutoff frequency: 0.25 Hz)\n",
      "- Upper passband edge: 45.00 Hz\n",
      "- Upper transition bandwidth: 11.25 Hz (-6 dB cutoff frequency: 50.62 Hz)\n",
      "- Filter length: 1651 samples (6.604 s)\n",
      "\n",
      "Not setting metadata\n",
      "231 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Using data from preloaded Raw for 231 events and 1250 original time points ...\n",
      "0 bad epochs dropped\n"
     ]
    }
   ],
   "source": [
    "sample_data = read_data(healthy_file_path[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(231, 19, 1250)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_data.shape  # number of epochs, channels, length of signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "control_epochs_array = [read_data(i) for i in healthy_file_path]\n",
    "patient_epochs_array = [read_data(i) for i in patient_file_path]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(231, 19, 1250)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "control_epochs_array[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14 14\n"
     ]
    }
   ],
   "source": [
    "control_epoch_labels = [[len(i)*[0]] for i in control_epochs_array]\n",
    "patient_epoch_labels = [[len(i)*[1]] for i in patient_epochs_array]\n",
    "print(len(control_epoch_labels), len(patient_epoch_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combining the data\n",
    "data_list = control_epochs_array + patient_epochs_array  \n",
    "label_list = control_epoch_labels + patient_epoch_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_list = [[i]*len(j) for i,j in enumerate(data_list)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(group_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "226"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(group_list[8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7201, 19, 1250)\n",
      "(1, 7201)\n",
      "(7201,)\n"
     ]
    }
   ],
   "source": [
    "data_array = np.vstack(data_list)\n",
    "label_array = np.hstack(label_list)\n",
    "group_array = np.hstack(group_list)\n",
    "print(data_array.shape)\n",
    "print(label_array.shape)\n",
    "print(group_array.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "def mean(data):\n",
    "    return np.mean(data,axis=-1)\n",
    "    \n",
    "def std(data):\n",
    "    return np.std(data,axis=-1)\n",
    "\n",
    "def ptp(data):\n",
    "    return np.ptp(data,axis=-1)\n",
    "\n",
    "def var(data):\n",
    "        return np.var(data,axis=-1)\n",
    "\n",
    "def minim(data):\n",
    "      return np.min(data,axis=-1)\n",
    "\n",
    "\n",
    "def maxim(data):\n",
    "      return np.max(data,axis=-1)\n",
    "\n",
    "def argminim(data):\n",
    "      return np.argmin(data,axis=-1)\n",
    "\n",
    "\n",
    "def argmaxim(data):\n",
    "      return np.argmax(data,axis=-1)\n",
    "\n",
    "def mean_square(data):\n",
    "      return np.mean(data**2,axis=-1)\n",
    "\n",
    "def rms(data): #root mean square\n",
    "      return  np.sqrt(np.mean(data**2,axis=-1))  \n",
    "\n",
    "def abs_diffs_signal(data):\n",
    "    return np.sum(np.abs(np.diff(data,axis=-1)),axis=-1)\n",
    "\n",
    "\n",
    "def skewness(data):\n",
    "    return stats.skew(data,axis=-1)\n",
    "\n",
    "def kurtosis(data):\n",
    "    return stats.kurtosis(data,axis=-1)\n",
    "\n",
    "def concatenate_features(data):\n",
    "    return np.concatenate((mean(data),std(data),ptp(data),var(data),minim(data),maxim(data),argminim(data),argmaxim(data),\n",
    "                          mean_square(data),rms(data),abs_diffs_signal(data),\n",
    "                          skewness(data),kurtosis(data)),axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tqdm import tqdm_notebook\n",
    "# features=[]\n",
    "# for data in tqdm_notebook(data_array):\n",
    "#     features.append(concatenate_features(data))\n",
    "# features=np.array(features)\n",
    "# features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = []\n",
    "for d in data_array:\n",
    "    features.append(concatenate_features(d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7201, 247)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_array = np.array(features)\n",
    "features_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GroupKFold, GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [7201, 1]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m param_grid \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclf__C\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;241m0.1\u001b[39m,\u001b[38;5;241m0.5\u001b[39m, \u001b[38;5;241m0.7\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m7\u001b[39m]}\n\u001b[0;32m      5\u001b[0m gscv \u001b[38;5;241m=\u001b[39m GridSearchCV(pipe,param_grid,cv\u001b[38;5;241m=\u001b[39mgkf,n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m12\u001b[39m)\n\u001b[1;32m----> 6\u001b[0m \u001b[43mgscv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures_array\u001b[49m\u001b[43m,\u001b[49m\u001b[43mlabel_array\u001b[49m\u001b[43m,\u001b[49m\u001b[43mgroups\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup_array\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Barzan\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py:1351\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1344\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1346\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1347\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1348\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1349\u001b[0m     )\n\u001b[0;32m   1350\u001b[0m ):\n\u001b[1;32m-> 1351\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Barzan\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:874\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[1;34m(self, X, y, **params)\u001b[0m\n\u001b[0;32m    869\u001b[0m \u001b[38;5;66;03m# Here we keep a dict of scorers as is, and only convert to a\u001b[39;00m\n\u001b[0;32m    870\u001b[0m \u001b[38;5;66;03m# _MultimetricScorer at a later stage. Issue:\u001b[39;00m\n\u001b[0;32m    871\u001b[0m \u001b[38;5;66;03m# https://github.com/scikit-learn/scikit-learn/issues/27001\u001b[39;00m\n\u001b[0;32m    872\u001b[0m scorers, refit_metric \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_scorers(convert_multimetric\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m--> 874\u001b[0m X, y \u001b[38;5;241m=\u001b[39m \u001b[43mindexable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    875\u001b[0m params \u001b[38;5;241m=\u001b[39m _check_method_params(X, params\u001b[38;5;241m=\u001b[39mparams)\n\u001b[0;32m    877\u001b[0m routed_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_routed_params_for_fit(params)\n",
      "File \u001b[1;32mc:\\Users\\Barzan\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\validation.py:476\u001b[0m, in \u001b[0;36mindexable\u001b[1;34m(*iterables)\u001b[0m\n\u001b[0;32m    457\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Make arrays indexable for cross-validation.\u001b[39;00m\n\u001b[0;32m    458\u001b[0m \n\u001b[0;32m    459\u001b[0m \u001b[38;5;124;03mChecks consistent length, passes through None, and ensures that everything\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    472\u001b[0m \u001b[38;5;124;03m    sparse matrix, or dataframe) or `None`.\u001b[39;00m\n\u001b[0;32m    473\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    475\u001b[0m result \u001b[38;5;241m=\u001b[39m [_make_indexable(X) \u001b[38;5;28;01mfor\u001b[39;00m X \u001b[38;5;129;01min\u001b[39;00m iterables]\n\u001b[1;32m--> 476\u001b[0m \u001b[43mcheck_consistent_length\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    477\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mc:\\Users\\Barzan\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\validation.py:430\u001b[0m, in \u001b[0;36mcheck_consistent_length\u001b[1;34m(*arrays)\u001b[0m\n\u001b[0;32m    428\u001b[0m uniques \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(lengths)\n\u001b[0;32m    429\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(uniques) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 430\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    431\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound input variables with inconsistent numbers of samples: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    432\u001b[0m         \u001b[38;5;241m%\u001b[39m [\u001b[38;5;28mint\u001b[39m(l) \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m lengths]\n\u001b[0;32m    433\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [7201, 1]"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression()\n",
    "gkf = GroupKFold(5)\n",
    "pipe = Pipeline([('scaler',StandardScaler()),('clf',clf)])\n",
    "param_grid = {'clf__C': [0.1,0.5, 0.7, 1, 3, 5, 7]}\n",
    "gscv = GridSearchCV(pipe,param_grid,cv=gkf,n_jobs=12)\n",
    "gscv.fit(features_array,label_array,groups=group_array)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clf=LogisticRegression()\n",
    "# gkf=GroupKFold(n_splits=5)\n",
    "# pipe=Pipeline([('scaler',StandardScaler()),('classifier',clf)])\n",
    "# param_grid = {'clf_C': [0.1,0.5, 0.7, 1, 3, 5, 7]}\n",
    "# gscv=GridSearchCV(pipe,param_grid,cv=gkf,n_jobs=16)\n",
    "# gscv.fit(features,label_array,groups=group_array)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
